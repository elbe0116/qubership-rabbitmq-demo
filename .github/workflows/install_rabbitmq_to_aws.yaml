name: Install RabbitMQ to AWS

on:
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment Name"
        required: false
        default: "dev"
      test_tags:
        description: "Test tags to run (e.g. smoke, backup, or empty for all)"
        required: false
        default: ""

jobs:
  Install-RabbitMQ:
    environment: "${{ github.event.inputs.environment }}"
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Listing downloaded files
        run: |
          pwd
          ls -R ./

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.32.0'

      - name: Setup Helm
        uses: azure/setup-helm@v3

      - name: Verify tools
        run: |
          kubectl version --client
          helm version
          aws --version

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Configure kubeconfig for EKS
        run: |
          aws eks update-kubeconfig --region ${{ vars.AWS_REGION || 'us-east-1' }} --name ${{ vars.AWS_CLUSTERNAME }}

      - name: Verify cluster access
        run: kubectl get nodes

      - name: Install RabbitMQ Cluster
        run: |
          # Uninstall Helm release first (cleans CRs and avoids finalizers blocking namespace deletion)
          echo "Uninstalling Helm release..."
          helm uninstall rabbitmq -n ${{ vars.RABBITMQ_INSTALL_NAMESPACE }} --wait --timeout=120s || true
          
          echo "Deleting namespace..."
          kubectl delete ns ${{ vars.RABBITMQ_INSTALL_NAMESPACE }} --timeout=180s --wait=false || true
          
          echo "Waiting for namespace deletion to complete..."
          for i in $(seq 1 90); do
            if ! kubectl get ns ${{ vars.RABBITMQ_INSTALL_NAMESPACE }} 2>/dev/null; then
              echo "Namespace deleted successfully"
              break
            fi
            
            NS_STATUS=$(kubectl get ns ${{ vars.RABBITMQ_INSTALL_NAMESPACE }} -o jsonpath='{.status.phase}' 2>/dev/null || echo "")
            echo "Waiting... ($i/90) - Namespace status: $NS_STATUS"
            
            # If stuck in Terminating, force delete (remove finalizers) like in monitoring workflow
            if [ "$i" -gt 30 ] && [ "$NS_STATUS" = "Terminating" ]; then
              echo "Namespace stuck in Terminating state, attempting to force delete..."
              echo "Removing finalizers from namespace resources..."
              kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n ${{ vars.RABBITMQ_INSTALL_NAMESPACE }} -o name 2>/dev/null | \
                xargs -n 1 kubectl patch -n ${{ vars.RABBITMQ_INSTALL_NAMESPACE }} -p '{"metadata":{"finalizers":[]}}' --type=merge 2>/dev/null || true
              echo "Removing finalizers from namespace..."
              kubectl get ns ${{ vars.RABBITMQ_INSTALL_NAMESPACE }} -o json | \
                jq '.spec.finalizers = []' | \
                kubectl replace --raw "/api/v1/namespaces/${{ vars.RABBITMQ_INSTALL_NAMESPACE }}/finalize" -f - || true
              sleep 5
            fi
            
            sleep 2
          done
          
          if kubectl get ns ${{ vars.RABBITMQ_INSTALL_NAMESPACE }} 2>/dev/null; then
            echo "ERROR: Namespace still exists after timeout!"
            kubectl get ns ${{ vars.RABBITMQ_INSTALL_NAMESPACE }} -o yaml
            exit 1
          fi
          
          echo "Creating namespace: ${{ vars.RABBITMQ_INSTALL_NAMESPACE }}"
          kubectl create namespace ${{ vars.RABBITMQ_INSTALL_NAMESPACE }}
          
          # AWS EKS values: TLS off, credentials, storage, resources, tests, backup daemon
          cat > /tmp/rabbitmq-aws-values.yaml << 'EOF'
          global:
            tls:
              enabled: false
            securityContext: {}
          operator:
            dockerImage: ghcr.io/netcracker/qubership-rabbitmq-operator:main
          rabbitmq:
            tls:
              enabled: false
            custom_params:
              rabbitmq_cluster_name: rabbitmq
              rabbitmq_vm_memory_high_watermark: "80%"
              rabbitmq_default_user: admin
              rabbitmq_default_password: admin123
            replicas: 3
            dockerImage: ghcr.io/netcracker/qubership-rabbitmq-image:main
            resources:
              requests:
                cpu: 100m
                memory: 512Mi
              limits:
                cpu: 500m
                memory: 1Gi
              storage: 5Gi
              storageclass: gp2
            # Required on EKS so PVC is writable by RabbitMQ process (avoids CrashLoopBackOff)
            fsGroup: 1000
            runAsUser: 1000
            securityContext: {}
          tests:
            runTests: true
            statusWritingEnabled: true
            rabbitIsManagedByOperator: true
            affinity: null
          telegraf:
            install: false
          backupDaemon:
            enabled: true
            image: ghcr.io/netcracker/qubership-rabbitmq-backup-daemon:main
            tls:
              enabled: false
            s3:
              enabled: true
              url: "https://s3.us-east-1.amazonaws.com"
              region: "us-east-1"
            securityContext: {}
          statusProvisioner:
            enabled: true
          EOF
          
          # Build helm arguments dynamically
          HELM_ARGS=""
          HELM_ARGS="$HELM_ARGS --set tests.tags=${{ github.event.inputs.test_tags }}"
          # Image from this fork (push.yml builds it); tag :main — Dockerfile base image branch is fixed in the Dockerfile
          GITHUB_GROUP=$(echo ${{ github.repository_owner }} | tr '[:upper:]' '[:lower:]')
          HELM_ARGS="$HELM_ARGS --set tests.dockerImage=ghcr.io/${GITHUB_GROUP}/qubership-rabbitmq-integration-tests:main"
          
          # Secrets — S3 credentials for test results upload (integration tests atpStorage)
          [ -n "${{ secrets.S3_AWS_ACCESS_KEY_ID }}" ] && HELM_ARGS="$HELM_ARGS --set tests.atpStorage.username=${{ secrets.S3_AWS_ACCESS_KEY_ID }}"
          [ -n "${{ secrets.S3_AWS_SECRET_ACCESS_KEY }}" ] && HELM_ARGS="$HELM_ARGS --set tests.atpStorage.password=${{ secrets.S3_AWS_SECRET_ACCESS_KEY }}"
          
          # Variables — ATP/S3 storage for integration tests (only add if set, like Kafka)
          [ -n "${{ vars.ATP_STORAGE_PROVIDER }}" ] && HELM_ARGS="$HELM_ARGS --set tests.atpStorage.provider=${{ vars.ATP_STORAGE_PROVIDER }}"
          [ -n "${{ vars.ATP_STORAGE_SERVER_URL }}" ] && HELM_ARGS="$HELM_ARGS --set tests.atpStorage.serverUrl=${{ vars.ATP_STORAGE_SERVER_URL }}"
          [ -n "${{ vars.ATP_STORAGE_SERVER_UI_URL }}" ] && HELM_ARGS="$HELM_ARGS --set tests.atpStorage.serverUiUrl=${{ vars.ATP_STORAGE_SERVER_UI_URL }}"
          [ -n "${{ vars.ATP_STORAGE_BUCKET }}" ] && HELM_ARGS="$HELM_ARGS --set tests.atpStorage.bucket=${{ vars.ATP_STORAGE_BUCKET }}"
          [ -n "${{ vars.AWS_REGION }}" ] && HELM_ARGS="$HELM_ARGS --set tests.atpStorage.region=${{ vars.AWS_REGION }}"
          [ -n "${{ vars.ATP_REPORT_VIEW_UI_URL }}" ] && HELM_ARGS="$HELM_ARGS --set tests.atpReportViewUiUrl=${{ vars.ATP_REPORT_VIEW_UI_URL }}"
          ENV_NAME="${{ vars.ENVIRONMENT_NAME }}"
          [ -z "$ENV_NAME" ] && ENV_NAME="${{ github.event.inputs.environment }}"
          [ -n "$ENV_NAME" ] && HELM_ARGS="$HELM_ARGS --set tests.environmentName=$ENV_NAME"
          
          # S3: use existing bucket from variable (required for backup daemon)
          S3_BUCKET="${{ vars.RABBITMQ_S3_BUCKET }}"
          [ -z "$S3_BUCKET" ] && echo "WARNING: RABBITMQ_S3_BUCKET not set, backup daemon may fail (NoSuchBucket)"
          HELM_ARGS="$HELM_ARGS --set backupDaemon.s3.bucket=${S3_BUCKET:-qstp-rabbitmq}"
          [ -n "${{ secrets.S3_AWS_ACCESS_KEY_ID }}" ] && HELM_ARGS="$HELM_ARGS --set backupDaemon.s3.keyId=${{ secrets.S3_AWS_ACCESS_KEY_ID }}"
          [ -n "${{ secrets.S3_AWS_SECRET_ACCESS_KEY }}" ] && HELM_ARGS="$HELM_ARGS --set backupDaemon.s3.keySecret=${{ secrets.S3_AWS_SECRET_ACCESS_KEY }}"
          [ -n "${{ vars.RABBITMQ_S3_URL }}" ] && HELM_ARGS="$HELM_ARGS --set backupDaemon.s3.url=${{ vars.RABBITMQ_S3_URL }}"
          
          echo "Helm additional args: $HELM_ARGS"
          
          helm install --namespace=${{ vars.RABBITMQ_INSTALL_NAMESPACE }} \
            --create-namespace \
            -f ./operator/charts/helm/rabbitmq/values.yaml \
            -f /tmp/rabbitmq-aws-values.yaml \
            rabbitmq ./operator/charts/helm/rabbitmq \
            $HELM_ARGS
          
          echo "Waiting for RabbitMQ cluster to be ready..."
          kubectl wait --for=condition=ready pod -l app=rmqlocal -n ${{ vars.RABBITMQ_INSTALL_NAMESPACE }} --timeout=600s || true
          
          echo "RabbitMQ installation completed"
          kubectl get pods -n ${{ vars.RABBITMQ_INSTALL_NAMESPACE }}
